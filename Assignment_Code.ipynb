{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18262b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required modules.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "layers = tf.keras.layers\n",
    "from sklearn import feature_selection, impute, preprocessing, model_selection, linear_model, ensemble, svm, metrics, neighbors\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0195cd",
   "metadata": {},
   "source": [
    "# MLP Classifier (Main Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c0248",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d000a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training CSV file into a Pandas dataframe.\n",
    "data = pd.read_csv('training2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant columns from dataframe and converting them to NumPy arrays.\n",
    "\n",
    "## this array contains only CaffeNet CNN features for all training samples.\n",
    "features_cnn = data.iloc[:,0:2048]\n",
    "features_cnn = features_cnn.to_numpy()\n",
    "\n",
    "## this array contains the class labels.\n",
    "labels = data.iloc[:,-2]\n",
    "labels = labels.to_numpy()\n",
    "\n",
    "## this array contains the confidence labels.\n",
    "conf = data.iloc[:,-1]\n",
    "conf = conf.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c952b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data in the training dataset.\n",
    "imputer = impute.KNNImputer()\n",
    "X_train = imputer.fit_transform(features_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing features by scaling them to the standard normal distribution (mean 0, variance 1).\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "fcnn_norm = scaler.fit_transform(features_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and validation sets, with 80% of the samples being training data and 20% being validation data.\n",
    "fcnn_train = X_train[:-500]\n",
    "labels_train = labels[:-500]\n",
    "conf_train = conf[:-500]\n",
    "\n",
    "fcnn_val = X_train[-500:]\n",
    "labels_val = labels[-500:]\n",
    "conf_val = conf[-500:]\n",
    "\n",
    "# Generating TensorFlow datasets for training and validation. \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((fcnn_train, labels_train, conf_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((fcnn_val, labels_val, conf_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41d3aa",
   "metadata": {},
   "source": [
    "## Building and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the MLP classifier using the Keras sequential API.\n",
    "def build_model():\n",
    "    layers_list = [layers.Dense(128, input_dim = 2048, activation = 'relu', activity_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                   layers.BatchNormalization(),\n",
    "                   layers.Dropout(0.4),\n",
    "                   layers.Dense(1, activation = 'sigmoid', activity_regularizer=tf.keras.regularizers.L2(0.01))]\n",
    "    model = keras.Sequential(layers_list)\n",
    "    return model\n",
    "\n",
    "mlp_model = build_model()\n",
    "\n",
    "mlp_model.compile(loss = 'binary_crossentropy', \n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.001), \n",
    "              weighted_metrics = ['accuracy'])\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', # EarlyStopping callback when validation loss stops decreasing, to prevent overfitting.\n",
    "                                         patience = 3, \n",
    "                                         mode = 'min')\n",
    "\n",
    "mlp_model.fit(train_dataset,\n",
    "                epochs = 20, \n",
    "                validation_data = val_dataset,\n",
    "                callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67607a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a schematic diagram of the MLP model for visualization.\n",
    "keras.utils.plot_model(mlp_model, \"mlp_model.png\", show_shapes=True)\n",
    "Image(retina=True, filename='mlp_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58855f12",
   "metadata": {},
   "source": [
    "## Predicting Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51339023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the test dataset CSV file into a Pandas dataframe, and extracting the CaffeNet CNN features into a NumPy array.\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "X_test = test_data.iloc[:,:2048]\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing feature values using KNNImputer, followed by scaling of the data to a standard normal distribution.\n",
    "X_test_imputed = imputer.fit_transform(X_test)\n",
    "X_test_norm = scaler.fit_transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test data, followed by rounding to the nearest integer to return 1s and 0s.\n",
    "predictions = mlp_model.predict(X_test_norm)\n",
    "predictions = np.rint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b93f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(predictions) # Converting label predictions array into Pandas dataframe.\n",
    "labels_df.to_csv('predictions1.csv') # Exporting predictions to CSV file for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd25e1",
   "metadata": {},
   "source": [
    "# Appendix (Auxiliary Code)\n",
    "\n",
    "* Not used for final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d41bb",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the GIST features, labels, and confidence labels from the training dataset, as seen above for the CNN features.\n",
    "\n",
    "## this array contains only gist features (256) for all 500 training samples.\n",
    "features_gist = data.iloc[:,2048:-2]\n",
    "features_gist = features_gist.to_numpy()\n",
    "\n",
    "## this array contains the class labels.\n",
    "labels = data.iloc[:,-2]\n",
    "labels = labels.to_numpy()\n",
    "\n",
    "## this array contains the confidence labels.\n",
    "conf = data.iloc[:,-1]\n",
    "conf = conf.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing feature values.\n",
    "\n",
    "imputer = impute.KNNImputer()\n",
    "\n",
    "gist_imputed = imputer.fit_transform(features_gist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling feature values to a standard normal distribution.\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "gist_norm = scaler.fit_transform(gist_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68928fc",
   "metadata": {},
   "source": [
    "# Univariate Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the SelectKBest method to obtain F-scores of all GIST features.\n",
    "\n",
    "selector = feature_selection.SelectKBest(feature_selection.f_classif, k = 'all')\n",
    "selected_features = selector.fit_transform(gist_norm, labels)\n",
    "\n",
    "# Sorting F-scores in descending order.\n",
    "\n",
    "sorted_scores = np.sort(selector.scores_)\n",
    "sorted_scores_descending = sorted_scores[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8546696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a chart of features and their F-scores.\n",
    "\n",
    "plt.plot(selector.scores_)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"F score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a chart of F-scores in descending order, with a vertical line representing the feature selection cutoff point.\n",
    "\n",
    "plt.plot(sorted_scores_descending)\n",
    "plt.axvline(x = 80, color = 'r')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"F score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score_indexes = (-selector.scores_).argsort()[:80] # Obtaining the indexes of the 80 most important features with the highest F-scores.\n",
    "f_score_indexes.sort()\n",
    "\n",
    "selected_gist = gist_norm[:,f_score_indexes] # Obtaining an array of selected features.\n",
    "print(f'Before feature selection, training data had shape: {gist_norm.shape}. After feature selection, training data has shape: {selected_features.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837d066",
   "metadata": {},
   "source": [
    "# Algorithm Selection\n",
    "\n",
    "Using 5x2-fold nested CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the classifier objects.\n",
    "\n",
    "clf_svm = svm.SVC(random_state=0)\n",
    "clf_rf = ensemble.RandomForestClassifier(random_state=0)\n",
    "clf_lr = linear_model.LogisticRegression(random_state=0)\n",
    "clf_knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "# Creating hyperparameter grids for hyperparameter tuning of each classifier.\n",
    "\n",
    "param_grid_svm = {'kernel' : ['rbf','poly','sigmoid'], \n",
    "                  'C' : np.power(10., np.arange(-4,4)),\n",
    "                  'gamma' : np.power(10., np.arange(-4,0))}\n",
    "\n",
    "param_grid_rf = {'criterion' : ['gini','log_loss'],\n",
    "                 'n_estimators' : [10, 100, 200, 500, 1000, 5000, 10000]}\n",
    "\n",
    "param_grid_lr = {'solver' : ['lbfgs','saga','liblinear'],\n",
    "                 'C' : np.power(10., np.arange(-4,4))}\n",
    "\n",
    "param_grid_knn = {'n_neighbors' : [1, 2, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the grid searches for hyperparameter tuning (inner fold of the nested CV).\n",
    "\n",
    "inner_fold = model_selection.StratifiedKFold(n_splits = 2, # Creating the inner folds for nested CV.\n",
    "                                             shuffle = True, \n",
    "                                             random_state = 0)\n",
    "\n",
    "gridcv = {}\n",
    "\n",
    "for grid, model, name in zip((param_grid_svm, param_grid_rf, param_grid_lr, param_grid_knn),\n",
    "                             (clf_svm, clf_rf, clf_lr, clf_knn),\n",
    "                             ('Support Vector Machine', 'Random Forest', 'Logistic Regression', 'K Nearest Neighbors')):\n",
    "    \n",
    "    result = model_selection.GridSearchCV(estimator = model, # Performing GridSearchCV for each classifier-hyperparameter grid set.\n",
    "                          param_grid = grid, \n",
    "                          scoring = 'accuracy', \n",
    "                          n_jobs = -1, \n",
    "                          cv = inner_fold, \n",
    "                          verbose = 0, \n",
    "                          refit = True)\n",
    "    \n",
    "    gridcv[name] = result # Stores results of tuned model in a dictionary as values, identified by its model name.\n",
    "    \n",
    "outer_fold = model_selection.StratifiedKFold(n_splits = 5, # Creating the outer folds for nested CV.\n",
    "                                             shuffle = True, \n",
    "                                             random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f048f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing the outer 5-fold cross-validation of the algorithm selection process. \n",
    "\n",
    "for name, model in gridcv.items():\n",
    "    \n",
    "    scores = model_selection.cross_validate(model,\n",
    "                                            X = selected_gist,\n",
    "                                            y = labels,\n",
    "                                            cv = outer_fold, \n",
    "                                            return_estimator = True,\n",
    "                                            n_jobs = -1)\n",
    "    \n",
    "    print('------------------------------------------------\\n')\n",
    "    print(f'Model Algorithm: {name}')\n",
    "    print('Inner Fold:')\n",
    "    \n",
    "    for i in range(scores['test_score'].shape[0]):\n",
    "        \n",
    "        print('\\n        Best ACC (avg. of inner test folds) %.2f%%' % (scores['estimator'][i].best_score_ * 100))\n",
    "        print('        Best parameters:', scores['estimator'][i].best_estimator_)\n",
    "        print('        ACC (on outer test fold) %.2f%%' % (scores['test_score'][i]*100))\n",
    "\n",
    "    print('\\n%s | outer ACC %.2f%% +/- %.2f' % \n",
    "          (name, scores['test_score'].mean() * 100, \n",
    "           scores['test_score'].std() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
